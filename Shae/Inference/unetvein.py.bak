import tensorflow.keras.backend as K
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.utils import load_img
from tensorflow.keras.metrics import MeanIoU
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow import Graph
from sklearn.model_selection import train_test_split
import re
import logging
import os
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Concatenate, Conv2DTranspose, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from keras.metrics import accuracy
import imageio.v2
import matplotlib.pyplot as plt
import math
from scipy import ndimage as ndi
import random
from sklearn.model_selection import KFold
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, Dense, GlobalAveragePooling2D, Multiply, Reshape








# Define custom metrics for binary segmentation
def iou(y_true, y_pred):
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(tf.maximum(y_true, y_pred))
    iou_score = intersection / (union + tf.keras.backend.epsilon())
    return iou_score

def dice_coef(y_true, y_pred):
    intersection = tf.reduce_sum(y_true * y_pred)
    dice = (2 * intersection) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + tf.keras.backend.epsilon())
    return dice
def binary_accuracy(y_true, y_pred):
    # Flatten the true and predicted masks to compute binary accuracy
    y_true_flat = tf.keras.backend.flatten(y_true)
    y_pred_flat = tf.keras.backend.flatten(y_pred)
    return tf.keras.metrics.binary_accuracy(y_true_flat, y_pred_flat)

def step_decay(epoch, initial_lr=0.001, drop_factor=0.5, epochs_drop=10):
    learning_rate = initial_lr * (drop_factor ** (epoch // epochs_drop))
    return learning_rate

def binary_unet(input_shape, num_classes, activation='sigmoid'):
    inputs = Input(input_shape)

    # Encoder
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Bottleneck with self-attention
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)
    conv4 = self_attention_block(conv4, 512)
    conv4 = Dropout(0.2)(conv4)

    # Decoder
    up1 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4)
    concat1 = concatenate([conv3, up1], axis=-1)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(concat1)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)
    conv5 = Dropout(0.2)(conv5)

    up2 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5)
    concat2 = concatenate([conv2, up2], axis=-1)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(concat2)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)
    conv6 = Dropout(0.2)(conv6)

    up3 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6)
    concat3 = concatenate([conv1, up3], axis=-1)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(concat3)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)
    conv7 = Dropout(0.2)(conv7)

    # Output
    outputs = Conv2D(num_classes, 1, activation=activation)(conv7)

    model = Model(inputs, outputs)
    
    return model

def self_attention_block(x, filters):
    # Channel attention
    channel_attention = x
    channel_attention = GlobalAveragePooling2D()(channel_attention)
    channel_attention = Dense(filters // 8, activation='relu')(channel_attention)
    channel_attention = Dense(filters, activation='sigmoid')(channel_attention)
    channel_attention = Reshape((1, 1, filters))(channel_attention)
    channel_attention = Multiply()([x, channel_attention])

    return channel_attention
